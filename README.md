# ğŸš€ AI SaaS Product â€“ Local LLM-Powered Analytics Platform

## ğŸ“Œ Project Description

This project is a **production-style AI SaaS application** that provides **data analysis and intelligent responses using a locally hosted Large Language Model (LLM)**. It is designed to simulate how modern SaaS platforms process logs, apply business rules, and use AI models to deliver structured insights through an API and UI.

The goal of this project is to **demonstrate real-world backend + AI + API design skills** that are highly relevant for **Software Engineer / AI Engineer / Backend roles**.

Key highlights:

* End-to-end system (Frontend â†’ Backend â†’ AI Model)
* Uses **local LLM (Ollama / LLaMA3)** â€“ no paid APIs
* Clean architecture with scalable components
* Interview-ready, real SaaS-style implementation

---

## ğŸ§  System Architecture

```
User (React UI)
     |
FastAPI API Gateway
     |
Log / Request Pre-Processor
     |        \
Rule Engine   Ollama (LLama3 LLM)
     |           |
Merged Structured Insights
     |
Response back to UI
```

---

## ğŸ› ï¸ Tech Stack & Why Chosen

### Backend

* **Python 3.10+** â€“ Industry standard, excellent AI ecosystem
* **FastAPI** â€“ High-performance, async-ready, production-grade APIs
* **Pydantic** â€“ Strong data validation and schema enforcement

### AI / LLM

* **Ollama (LLaMA 3)** â€“

  * Runs fully **offline**
  * No API cost
  * Demonstrates real LLM integration skills

### Frontend

* **React.js** â€“ Most in-demand frontend framework
* **Axios** â€“ Clean API communication

### DevOps / Tooling

* **Docker** â€“ Reproducible & scalable deployment
* **Git & GitHub** â€“ Version control and collaboration

âœ… This stack mirrors **real SaaS company architecture** used in Bangalore startups and MNCs.

---

## âš™ï¸ Step-by-Step Setup Guide

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install & Run Ollama

```bash
# Install Ollama (Linux/Mac)
curl -fsSL https://ollama.com/install.sh | sh

# Pull LLaMA 3 model
ollama pull llama3

# Start Ollama server
ollama serve
```

### 4ï¸âƒ£ Start FastAPI Server

```bash
uvicorn main:app --reload
```

API will be available at:

```
http://127.0.0.1:8000
```

Swagger Docs:

```
http://127.0.0.1:8000/docs
```

---

### 5ï¸âƒ£ Frontend Setup

```bash
cd frontend
npm install
npm start
```

Frontend runs at:

```
http://localhost:3000
```

---

## ğŸ“¸ Screenshots / Demo

> ğŸ“Œ Add the following before applying for jobs:

* ğŸ“· Screenshots of UI & API response
* ğŸ¥ 1â€“2 minute Loom / YouTube demo video

Example:

```md
![Dashboard](screenshots/dashboard.png)
```

Live Demo (optional):

```
https://your-demo-link.com
```

---

## ğŸ‘¨â€ğŸ’» My Individual Contributions

* Designed **end-to-end SaaS architecture** from scratch
* Built **FastAPI backend** with clean routing & validation
* Integrated **local LLM (Ollama + LLaMA3)** for AI responses
* Implemented **rule engine + AI hybrid decision flow**
* Created **React UI** for real-time interaction
* Containerized services using **Docker**
* Wrote production-level **README & setup documentation**

---

## ğŸ¯ Why This Project Stands Out

âœ… Uses **real AI (LLM)**, not mock logic
âœ… No paid APIs â€“ fully local & cost-free
âœ… SaaS-style architecture interviewers expect
âœ… Clear separation of concerns
âœ… Ready to scale and extend

---

## ğŸ“Œ Future Enhancements

* Authentication (JWT)
* Multi-user support
* Database integration (PostgreSQL)
* Model switching (Mistral, Gemma)
* Cloud deployment (AWS / GCP)

---

## ğŸ“ Contact

**Divith Raju**
ğŸ“ B.Tech â€“ Artificial Intelligence & Data Science (2026)
ğŸ“ Bangalore, India
ğŸ”— GitHub: [https://github.com/divithraju](https://github.com/divithraju)

---

